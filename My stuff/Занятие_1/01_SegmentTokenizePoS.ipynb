{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "01_SegmentTokenizePoS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0zmDYwWleDZt",
        "KQq4iPN8eDZ2",
        "DAbQc-VzeDab",
        "UmRbpXNTB72C",
        "EvFlNWZ1eDbA",
        "DPMiIDMEeDbE",
        "dop3xhkteDbN",
        "gqd_IVNNeDbS",
        "BN71xIw6eDbY",
        "NDIt0zSKeDbg",
        "PhBHsBTyeDbo"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/root-epifit/NLP-Sber-Winter-2022/blob/main/01_SegmentTokenizePoS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scB5smameDYo"
      },
      "source": [
        "# Настройка\n",
        "Все зависимости перечислены в ячейке ниже. Кроме того, есть ещё дополнительные данные (opencorpora, например). Они тоже скачиваются в первых ячейках."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PoyIMujeDYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cc7e84-be82-4368-c190-feb4ac05e9d9"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "nltk>=3.4.5\n",
        "razdel>=0.4.0\n",
        "rusenttokenize>=0.0.5\n",
        "b-labs-models>=2017.8.22\n",
        "lxml>=4.2.1\n",
        "spacy>=2.1.4\n",
        "pymystem3>=0.2.0\n",
        "rnnmorph>=0.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ysaxzVeDYt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5c8e645-9e55-46d5-d8a3-51ee2cd7da0d"
      },
      "source": [
        "import sys\n",
        "!pip install --user --upgrade --force-reinstall -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk>=3.4.5\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 686 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 696 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 706 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting razdel>=0.4.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting rusenttokenize>=0.0.5\n",
            "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting b-labs-models>=2017.8.22\n",
            "  Downloading b_labs_models-2017.8.22-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 28.2 MB/s \n",
            "\u001b[?25hCollecting lxml>=4.2.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 45.6 MB/s \n",
            "\u001b[?25hCollecting spacy>=2.1.4\n",
            "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting pymystem3>=0.2.0\n",
            "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting rnnmorph>=0.4.0\n",
            "  Downloading rnnmorph-0.4.0.tar.gz (10.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5 MB 27.3 MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "  Downloading regex-2021.8.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 57.0 MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.5-cp37-cp37m-manylinux2014_x86_64.whl (35 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 851 kB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-58.1.0-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 56.5 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 20.5 MB/s \n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (20 kB)\n",
            "Collecting jinja2\n",
            "  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 42.5 MB/s \n",
            "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
            "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.9\n",
            "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 68.5 MB/s \n",
            "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 43.8 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.15.0\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 403 kB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 38.2 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-21.0-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting scipy>=0.18.1\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.18.1\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow>=1.1.0\n",
            "  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 10 kB/s \n",
            "\u001b[?25hCollecting keras>=2.0.6\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 28.2 MB/s \n",
            "\u001b[?25hCollecting pymorphy2>=0.8\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting russian-tagsets==0.6\n",
            "  Downloading russian-tagsets-0.6.tar.gz (23 kB)\n",
            "Collecting jsonpickle>=0.9.4\n",
            "  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
            "Collecting pyparsing>=2.0.2\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 17.3 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 52.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.6-py3-none-any.whl (37 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting keras-preprocessing~=1.1.2\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard~=2.6\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 46.6 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.14.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 75.3 MB/s \n",
            "\u001b[?25hCollecting h5py~=3.1.0\n",
            "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
            "  Downloading grpcio-1.40.0-cp37-cp37m-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 37.2 MB/s \n",
            "\u001b[?25hCollecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "Collecting google-pasta~=0.2\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.15.0\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 345 kB/s \n",
            "\u001b[?25hCollecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 46.5 MB/s \n",
            "\u001b[?25hCollecting six~=1.15.0\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting astunparse~=1.6.3\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting wheel~=0.35\n",
            "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting opt-einsum~=3.3.0\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 71.1 MB/s \n",
            "\u001b[?25hCollecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 30.1 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 34.7 MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 58.2 MB/s \n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
            "Building wheels for collected packages: rnnmorph, russian-tagsets, docopt, clang, termcolor, wrapt\n",
            "  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rnnmorph: filename=rnnmorph-0.4.0-py3-none-any.whl size=10521035 sha256=50f3f84faab2c0d390fae9a6bda097cfad06b41498790be91492d16a98d91bc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/ec/06/de2b6e347249349451430b4c7d2f20a460a5c4af3838911637\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-py3-none-any.whl size=24637 sha256=fe874aaea262dc4be625c5b5bd812a832fb0b517f0ef6fa0e1774a04e6ef0406\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b4/26/9c17a7cdcfc6b8cf43111312f3e7c6abb8e583599e37c422f8\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=737f1fe59af35cfed43ac7aa173478be000ec9293f47716ca197d292654fcbdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=19ed1eaeefcd082706e05cb7db3b1808fbc58805c3db5a2500dbdfe4fd050a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=d0b1d2f41fe8667b5500f90df7c42baa081a99ce956cdb70331786d1496cc9f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68711 sha256=79996d27551b0374076b1e00f21c7e82024127da747f994c674845b091ac0fe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built rnnmorph russian-tagsets docopt clang termcolor wrapt\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, murmurhash, markdown, grpcio, google-auth-oauthlib, cymem, click, catalogue, cached-property, absl-py, wrapt, wasabi, typer, tqdm, threadpoolctl, termcolor, tensorflow-estimator, tensorboard, srsly, smart-open, scipy, regex, pyparsing, pymorphy2-dicts-ru, pydantic, preshed, opt-einsum, MarkupSafe, keras-preprocessing, keras, joblib, h5py, google-pasta, gast, flatbuffers, docopt, dawg-python, clang, blis, astunparse, thinc, tensorflow, spacy-legacy, scikit-learn, russian-tagsets, python-crfsuite, pymorphy2, pathy, packaging, nltk, jsonpickle, jinja2, spacy, rusenttokenize, rnnmorph, razdel, pymystem3, lxml, b-labs-models\n",
            "\u001b[33m  WARNING: The script normalizer is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script wheel is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script markdown_py is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script pymorphy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script pathy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script spacy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script razdel-ctl is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.1 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.0.1 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed MarkupSafe-2.0.1 absl-py-0.14.0 astunparse-1.6.3 b-labs-models-2017.8.22 blis-0.7.4 cached-property-1.5.2 cachetools-4.2.2 catalogue-2.0.6 certifi-2021.5.30 charset-normalizer-2.0.6 clang-5.0 click-8.0.1 cymem-2.0.5 dawg-python-0.7.2 docopt-0.6.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.40.0 h5py-3.1.0 idna-3.2 importlib-metadata-4.8.1 jinja2-3.0.1 joblib-1.0.1 jsonpickle-2.0.0 keras-2.6.0 keras-preprocessing-1.1.2 lxml-4.6.3 markdown-3.3.4 murmurhash-1.0.5 nltk-3.6.3 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 packaging-21.0 pathy-0.6.0 preshed-3.0.5 protobuf-3.18.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.8.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pymystem3-0.2.0 pyparsing-2.4.7 python-crfsuite-0.9.7 razdel-0.5.0 regex-2021.8.28 requests-2.26.0 requests-oauthlib-1.3.0 rnnmorph-0.4.0 rsa-4.7.2 rusenttokenize-0.0.5 russian-tagsets-0.6 scikit-learn-0.24.2 scipy-1.7.1 setuptools-58.1.0 six-1.15.0 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 thinc-8.0.10 threadpoolctl-2.2.0 tqdm-4.62.3 typer-0.4.0 typing-extensions-3.7.4.3 urllib3-1.26.6 wasabi-0.8.2 werkzeug-2.0.1 wheel-0.37.0 wrapt-1.12.1 zipp-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pkg_resources",
                  "pyparsing",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocGvOjsE9GKh"
      },
      "source": [
        "# Restart kernel\n",
        "\n",
        "import os\n",
        "os._exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FJ2-w6keDYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e3c20a-3030-4eb0-c184-9aef554b3514"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dqa3TH-eDYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4819e8d3-5f15-4429-8a72-674dddd5d284"
      },
      "source": [
        "import sys\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /root/.local/lib/python3.7/site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.26.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (58.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /root/.local/lib/python3.7/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /root/.local/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /root/.local/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /root/.local/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/.local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/.local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/.local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/.local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /root/.local/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /root/.local/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/.local/lib/python3.7/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A7fL9saeDYz"
      },
      "source": [
        "### Дополнительные данные\n",
        "\n",
        "Opencorpora: 31 Мб по сети, 530 Мб в распакованном виде"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BnW_XqBeDY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1ee0e8-2da0-4556-dd46-8ca4e67a6343"
      },
      "source": [
        "!wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 14:37:01--  http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2\n",
            "Resolving opencorpora.org (opencorpora.org)... 46.4.87.221\n",
            "Connecting to opencorpora.org (opencorpora.org)|46.4.87.221|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32759089 (31M) [application/x-bzip2]\n",
            "Saving to: ‘annot.opcorpora.xml.bz2’\n",
            "\n",
            "annot.opcorpora.xml 100%[===================>]  31.24M  17.0MB/s    in 1.8s    \n",
            "\n",
            "2021-09-22 14:37:03 (17.0 MB/s) - ‘annot.opcorpora.xml.bz2’ saved [32759089/32759089]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wX8NtUveDY2"
      },
      "source": [
        "!bzip2 -d annot.opcorpora.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiKkGUnbeDY4"
      },
      "source": [
        "### Тестовые примеры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdqD-e7beDY5"
      },
      "source": [
        "example1 = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
        "example2 = \"\"\"\n",
        "    An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.\n",
        "    Assumption Hall, the first student dormitory, was opened in 1954,\n",
        "    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.\n",
        "    It was during the tenure of F. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.\n",
        "\"\"\"\n",
        "example3 = \"\"\"\n",
        "    А что насчёт русского языка? Хорошо ли сегментируются имена?\n",
        "    Ай да А.С. Пушкин! Ай да сукин сын!\n",
        "    «Как же так?! Захар...» — воскликнут Пронин.\n",
        "    - \"Так в чем же дело?\" - \"Не ра-ду-ют\".\n",
        "    И т. д. и т. п. В общем, вся газета.\n",
        "    Православие... более всего подходит на роль такой идеи...\n",
        "    Нефть за $27/барр. не снится.\n",
        "\"\"\"\n",
        "example4 = \"\"\"\n",
        "    Кружка-термос на 0.5л (50/64 см³, 516;...) стоит $3.88\n",
        "\"\"\"\n",
        "example5 = \"\"\"\n",
        "    Good muffins cost $3.88 in New York.  Please buy me two of them. Thanks.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cqtzsw4eDY7"
      },
      "source": [
        "# Сегментация предложений\n",
        "Первая задача - разбиение текста на предложения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGebcOPseDY8"
      },
      "source": [
        "### Экперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiEkV_MdeDY9"
      },
      "source": [
        "##### NLTK - Natural Language Toolkit\n",
        "Популярная платформа для анализа текстов. Особенно хорошо работает для английского. В основном не содержит ничего из машинного обучения, только старые добрые правила."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42DWKe5FeDY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5112a64-ff99-45b2-a0d7-d96b6403d977"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(example1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"this's a sent tokenize test.\",\n",
              " 'this is sent two.',\n",
              " 'is this sent three?',\n",
              " 'sent 4 is cool!',\n",
              " 'Now it’s your turn.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWpr39pPeDZA"
      },
      "source": [
        "А вот тут что-то пошло не так"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8bXtc-6eDZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f4c6ea-4a85-4825-c6ca-359f2685e9bd"
      },
      "source": [
        "sent_tokenize(example2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    An ambitious campus expansion plan was proposed by Fr.',\n",
              " 'Vernon F. Gallagher in 1952.',\n",
              " 'Assumption Hall, the first student dormitory, was opened in 1954,\\n    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.',\n",
              " 'It was during the tenure of F. Henry J. McAnulty that Fr.',\n",
              " \"Gallagher's ambitious plans were put to action.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dpmBe70eDZE"
      },
      "source": [
        "А что насчёт русского языка?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNCCwnzeDZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e02989f-cb56-450f-e349-058f25b10673"
      },
      "source": [
        "sent_tokenize(example3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С.',\n",
              " 'Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п. В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
              " 'не снится.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rWZ0bF4eDZH"
      },
      "source": [
        "https://github.com/Mottl/ru_punkt\n",
        "\n",
        "Data for sentence tokenization was taken from 3 sources:\n",
        "\n",
        "  * Articles from Russian Wikipedia (about 1 million sentences)\n",
        "  * Common Russian abbreviations from Russian orthographic dictionary, edited by V. V. Lopatin;\n",
        "  * Generated names initials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwr9BaZ3eDZI"
      },
      "source": [
        "sent_tokenize(example3, language=\"russian\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erWAIVigeDZL"
      },
      "source": [
        "https://github.com/natasha/razdel\n",
        "\n",
        "razdel старается разбивать текст на предложения и токены так, как это сделано в 4 датасетах: SynTagRus, OpenCorpora, ГИКРЯ и РНК из репозитория morphoRuEval-2017.\n",
        "\n",
        "В основном это новостные тексты и литература. Правила razdel заточены под них.\n",
        "\n",
        "На текстах другой тематики (социальные сети, научные статьи) библиотека может работать хуже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY1qpHh6eDZL"
      },
      "source": [
        "from razdel import sentenize\n",
        "list(sentenize(example3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6DDGryLeDZP"
      },
      "source": [
        "https://github.com/deepmipt/ru_sentence_tokenizer\n",
        "    \n",
        "A simple and fast rule-based sentence segmentation. Tested on OpenCorpora and SynTagRus datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6iYyZ1TeDZQ"
      },
      "source": [
        "\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "ru_sent_tokenize(example3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwNAtibneDZS"
      },
      "source": [
        "### Бенчмарки\n",
        "Много вариантов... Нужно измерять"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwae8cmleDZT"
      },
      "source": [
        "# WARNING: RAM bound task, XML parsing is expensive\n",
        "# Similar to https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb\n",
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "# \\W -> Any non-word character\n",
        "RE_ENDS_WITH_PUNCT = re.compile(r\".*\\W$\")\n",
        "\n",
        "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
        "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
        "singles = []\n",
        "compounds = []\n",
        "s2 = sentences.pop().strip()\n",
        "singles.append(s2)\n",
        "while sentences:\n",
        "    s1 = sentences.pop().strip()\n",
        "    singles.append(s1)\n",
        "    if RE_ENDS_WITH_PUNCT.match(s1) and not s1.endswith(':') and not s2.startswith('—'):\n",
        "        compounds.append((s1, s2))\n",
        "    s2 = s1\n",
        "        \n",
        "print(f'Read {len(singles)} sentences from {OPENCORPORA_FILE}')\n",
        "        \n",
        "del sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2havDX5seDZV"
      },
      "source": [
        "def check_sent_tokenizer(tokenizer, singles, compounds):\n",
        "    correct_count_in_singles = 0\n",
        "    for sentence in singles:\n",
        "        correct_count_in_singles += len(tokenizer(sentence)) == 1\n",
        "\n",
        "    correct_count_in_compounds = 0\n",
        "    for s1, s2 in compounds:\n",
        "        correct_count_in_compounds += tokenizer(s1 + ' ' + s2) == [s1, s2]\n",
        "\n",
        "    return (correct_count_in_singles / len(singles), correct_count_in_compounds / len(compounds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvWnP6bEeDZY"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(sent_tokenize, singles, compounds)\n",
        "print(f'sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f9m7nnneDZc"
      },
      "source": [
        "russian_sent_tokenize = lambda s : sent_tokenize(s, language=\"russian\")\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(russian_sent_tokenize, singles, compounds)\n",
        "print(f'russian sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClqfLRveDZh"
      },
      "source": [
        "from razdel import sentenize\n",
        "razdel_sent_tokenize = lambda text : [s.text for s in sentenize(text)]\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(razdel_sent_tokenize, singles, compounds)\n",
        "print(f'razdel scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuAOcawmeDZo"
      },
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "deepmipt_sent_tokenize = ru_sent_tokenize\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(deepmipt_sent_tokenize, singles, compounds)\n",
        "print(f'deepmipt scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tQJPC51eDZs"
      },
      "source": [
        "Аналогичные бенчмарки:\n",
        "- https://github.com/natasha/razdel/blob/master/eval.ipynb\n",
        "- https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zmDYwWleDZt"
      },
      "source": [
        "### Задание 1: \"Кирпич\"\n",
        "Скачайте предложенный текст. Найдите первое предложение, которое отличается в разбиениях, порождённых rusenttokenize и razdel. Верните номер этого предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKi00K-seDZu"
      },
      "source": [
        "!wget https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzXHecqeDZy"
      },
      "source": [
        "from razdel import sentenize\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "\n",
        "\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    \n",
        "def get_first_different_sentence(text: str) -> int:\n",
        "    # YOUR CODE HERE\n",
        "    return -1\n",
        "\n",
        "assert get_first_different_sentence(text) == 329"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQq4iPN8eDZ2"
      },
      "source": [
        "### Задание 2: Lazy baseline\n",
        "Напишите свой sent_tokenize, который будет делить предложения только по точкам, восклицательным и вопросительным знакам. Измерьте для него время работы и метрики на opencorpora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47iRVTmqeDZ3"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def my_sent_tokenize(text):\n",
        "    # YOUR CODE HERE\n",
        "    return []\n",
        "\n",
        "assert my_sent_tokenize(example1) == sent_tokenize(example1)\n",
        "%time singles_score, compounds_score = 0.0, 0.0 # YOUR CODE HERE\n",
        "assert singles_score >= 0.85\n",
        "print(f'your scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvte2OBUeDZ5"
      },
      "source": [
        "# Токенизация\n",
        "\n",
        "Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0BjSKCneDZ5"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(example5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30jl5PpZeDZ8"
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD3EUO5WeDZ_"
      },
      "source": [
        "Они умеют выдавать индексы начала и конца каждого токена:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2z_ZybxeDaA"
      },
      "source": [
        "from nltk import tokenize\n",
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "print(list(wh_tok.span_tokenize(example5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4HjdWCceDaE"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzF35cOeDaG"
      },
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQCmuQpGeDaK"
      },
      "source": [
        "import spacy\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "doc = spacy_nlp(example5, disable=[\"parser\"])\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg9w4wNSeDaN"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(example4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVvJnHfKeDaP"
      },
      "source": [
        "from razdel import tokenize\n",
        "list(tokenize(example4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVCUdgpOeDaR"
      },
      "source": [
        "### Задание 3: Diff\n",
        "Напишите функцию, которая будет выводить разницу между токенизацией razdel'а и nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gRApSXtKeDaS"
      },
      "source": [
        "from difflib import SequenceMatcher # USE THIS\n",
        "from razdel import tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    \n",
        "def get_tokenization_differences(text: str) -> int:\n",
        "    differences = []\n",
        "    # YOUR CODE HERE\n",
        "    return differences\n",
        "\n",
        "assert len(get_tokenization_differences(text)) == 613"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IPBbpoVeDaU"
      },
      "source": [
        "# Стоп-слова и пунктуация\n",
        "\n",
        "Стоп-слова - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конкретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CwruKoseDaU"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PVbyxz-eDaW"
      },
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rksBLbveDaZ"
      },
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAbQc-VzeDab"
      },
      "source": [
        "### Задание 4: Стоп-слова from scratch\n",
        "Постройте свой список стоп-слов на основе Opencorpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGRerPDseDac"
      },
      "source": [
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
        "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
        "print(f'Read {len(sentences)} sentences from {OPENCORPORA_FILE}')\n",
        "\n",
        "# YOUR CODE HERE\n",
        " \n",
        "del sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGQgO59KZ9G"
      },
      "source": [
        "# Стемминг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZsLS4NQKW77"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from razdel import tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\") \n",
        "print([stemmer.stem(token.text) for token in tokenize(example3)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6ufubEeDae"
      },
      "source": [
        "# Лемматизация и морфологический анализ\n",
        "\n",
        "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n",
        "* Мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n",
        "* Некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть два хороших лемматизатора: mystem и pymorphy. С pymorphy всё сразу понятно.\n",
        "\n",
        "Но как работать с Mystem:\n",
        "* Можно скачать mystem и запускать из терминала с разными параметрами\n",
        "* pymystem3 - обертка для питона, работает медленнее, но это удобно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfork2jueDaf"
      },
      "source": [
        "## Mystem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ZoKAS5eDag"
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem_analyzer = Mystem()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7McqUjduMYxR"
      },
      "source": [
        "!chmod +x /root/.local/bin/mystem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsSzbzVWeDai"
      },
      "source": [
        "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "\n",
        "    mystem_bin - путь к mystem, если их несколько\n",
        "    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
        "\n",
        "Можно просто лемматизировать текст:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK6iAkmWeDai"
      },
      "source": [
        "print(mystem_analyzer.lemmatize(example3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZg3McyqeDal"
      },
      "source": [
        "А можно получить грамматическую информацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F_PNsqneDal"
      },
      "source": [
        "mystem_analyzer.analyze(example3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJH0dy1YeDan"
      },
      "source": [
        "## Pymorphy\n",
        "\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tsjLpjeDao"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOudNTMJeDap"
      },
      "source": [
        "pymorphy2_analyzer.parse(\"мою\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmRbpXNTB72C"
      },
      "source": [
        "### Задание 5: Анализ частей речи\n",
        "\n",
        "Используя pymorphy2, определите топ-10 самых частотных существительных и глаголов в тексте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My2CZHRwCODO"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "pymorphy2_analyzer = MorphAnalyzer()\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qJvkxveDas"
      },
      "source": [
        "## mystem vs. pymorphy\n",
        "\n",
        "1) Mystem работает невероятно медленно под windows на больших текстах.\n",
        "\n",
        "2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO55wLFkeDas"
      },
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
        "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApdUILveDax"
      },
      "source": [
        "## Rnnmorph\n",
        "Обёртка над pymorphy с разрешением омонимии\n",
        "\n",
        "https://github.com/IlyaGusev/rnnmorph\n",
        "\n",
        "https://habr.com/ru/post/339954/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUqNO1leDa8"
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "from razdel import tokenize\n",
        "\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "homonym = \"Косил косой косой косой\"\n",
        "print(predictor.predict([t.text for t in tokenize(homonym)])[1])\n",
        "print(predictor.predict([t.text for t in tokenize(homonym)])[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeFOWPD17PC3"
      },
      "source": [
        "## GramEval-2020\n",
        "\n",
        "Соревнование по определению морфологических характеристик, определению синтаксических зависимостей и лемматизации. Готовых инструментов не получилось, но весь код всех конкурсантов доступен.\n",
        "* https://github.com/dialogue-evaluation/GramEval2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFlNWZ1eDbA"
      },
      "source": [
        "### Задание 6: Формат\n",
        "\n",
        "Используя стандартные инструменты переведите корпус htbg.txt в формат CoNLL-U.\n",
        "Используйте следующие колонки: \n",
        "    1. Номер предложения в тексте\n",
        "    2. Токен в том виде, в котором он встретился в тексте\n",
        "    3. Лемма токена\n",
        "    4. POS-таг токена\n",
        "    5. Вектор грамматических значений токена\n",
        "    6. Целевая метка (сделайте метку везде OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfew_CvgeDbB"
      },
      "source": [
        "# Regex 101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDYSLv67eDbB"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPMiIDMEeDbE"
      },
      "source": [
        "#### match\n",
        "ищет по заданному шаблону в начале строки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ4RY9dveDbF"
      },
      "source": [
        "result = re.match('ab+c.', 'abcdefghijkabcabc') # ищем по шаблону 'ab+c.' \n",
        "print (result) # совпадение найдено:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2qN9Q1keDbH"
      },
      "source": [
        "print(result.group(0)) # выводим найденное совпадение"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqg4wISLeDbK"
      },
      "source": [
        "result = re.match('abc.', 'abdefghijkabcabc')\n",
        "print(result) # совпадение не найдено"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dop3xhkteDbN"
      },
      "source": [
        "#### search\n",
        "ищет по всей строке, возвращает только первое найденное совпадение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsP6-GN-eDbO"
      },
      "source": [
        "result = re.search('ab+c.', 'aefgabchijkabcabc') \n",
        "print(result) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqd_IVNNeDbS"
      },
      "source": [
        "#### findall\n",
        "возвращает список всех найденных совпадений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox0yWO57eDbT"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ncd6YPeDbV"
      },
      "source": [
        "Вопросы: \n",
        "1) почему нет последнего abc?\n",
        "2) почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN71xIw6eDbY"
      },
      "source": [
        "#### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsWWmhUKeDbZ"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGR8y8uCeDbc"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alEOle-XeDbd"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDIt0zSKeDbg"
      },
      "source": [
        "#### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79CKEy5eDbk"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhBHsBTyeDbo"
      },
      "source": [
        "#### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3PuZ3feDbp"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_gSGRhKeDbu"
      },
      "source": [
        "# Ваш код"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}